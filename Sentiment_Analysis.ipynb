{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdPg6jPyc9p0SWK9TELpmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitprajapati1/Deep_and_Machine_Learning_Projects/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGjRTSH7dMCA"
      },
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n9QDcrMdhZB"
      },
      "source": [
        "import numpy\r\n",
        "from numpy import array\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM, Dropout\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.models import load_model\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WegvXE-dlYP"
      },
      "source": [
        "numpy.random.seed(7)\r\n",
        "\r\n",
        "# Load the dataset but only keep the top n words and zero out the rest i.e keep vocabulary size as 5000\r\n",
        "top_words = 5000 #vocabulary_size = 5000\r\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\r\n",
        "\r\n",
        "'''Inspect a sample review and its label.Note that the review is stored as a sequence of integers. These are word IDs that \r\n",
        "have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive).'''\r\n",
        "print('---review---')\r\n",
        "print(X_train[6])\r\n",
        "print('---label---')\r\n",
        "print(y_train[6])\r\n",
        "\r\n",
        "'''We can use the dictionary returned by imdb.get_word_index() to map the review back to the original words.'''\r\n",
        "word2id = imdb.get_word_index()\r\n",
        "id2word = {i: word for word, i in word2id.items()}\r\n",
        "print('---review with words---')\r\n",
        "print([id2word.get(i, ' ') for i in X_train[6]])\r\n",
        "print('---label---')\r\n",
        "print(y_train[6])\r\n",
        "\r\n",
        "print(word2id)\r\n",
        "\r\n",
        "print(id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De9nSO5rd2EG"
      },
      "source": [
        "print('Maximum review length: {}'.format(\r\n",
        "len(max((X_train + X_test), key=len))))\r\n",
        "\r\n",
        "print('Minimum review length: {}'.format(\r\n",
        "len(min((X_train + X_test), key=len))))\r\n",
        "\r\n",
        "'''In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to maximum words by truncating \r\n",
        "longer reviews and padding shorter reviews with a null value (0). We can accomplish this task using the pad_sequences() function in Keras. Here, setting max_review_length \r\n",
        "to 500.'''\r\n",
        "\r\n",
        "max_review_length = 500\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\r\n",
        "\r\n",
        "'''Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_review_length, and our output is a binary sentiment \r\n",
        "label (0 or 1).\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i64MmRtWd9IB"
      },
      "source": [
        "embedding_vecor_length = 32\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "'''We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics \r\n",
        "we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’.'''\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwcA4iyqeFEr"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)\r\n",
        "\r\n",
        "#Calculate Accuracy\r\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "\r\n",
        "#Run these codes first in order to install the necessary libraries and perform authorization\r\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\r\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\r\n",
        "!apt-get update -qq 2>&1 > /dev/null\r\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLo6u9xJeP20"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}